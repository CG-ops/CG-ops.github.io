<!DOCTYPE html>

<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Zilu Guo's home page"> 
  
  <link href="./files/wfdoc.css" rel="stylesheet" type="text/css"> 
  <title>Zilu Guo's Homepage</title> 
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Claire (Zilu) Guo </h1></div>
        <!-- <h3>Mphil Candidate</h3> -->
        <p>The Chinese University of Hong Kong (Shenzhen)
        <!-- <br>School of Automation, -->
        <!-- <br>USTC -->
        <!-- <br>Shenzhen, China -->
        <br>
        <br> Email:  
        <a href="mailto:ziluguo1@link.cuhk.edu.cn"> ziluguo1@link.cuhk.edu.cn</a>; <a href="mailto:guozilu.claire@gmail.com">guozilu.claire@gmail.com</a>
        <!-- <a href="mailto:guozilu.claire@gmail.com">guozilu.claire@gmail.com</a>;  -->
        <!-- <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:guozilu.claire@gmail.com">guozilu.claire@gmail.com</a>;  -->
        <br> Google Scholar:  
        <a href="https://scholar.google.com/citations?user=TmGuqagAAAAJ&hl=en"> Google Scholar Link</a>
        <br> Wechat:  
        <a href=""> nature_claireG</a>
        <!-- <br> Github:  -->
        <!-- <a href="https://github.com/ClaireGZL/">https://github.com/ClaireGZL/</a>  -->
        <br><br></p>
      </td>
      <td>
        <div>
          <img width="260" src="./files/zhuyeimg.jpg" border="0">
          <!-- <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> -->
        </div>
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">


  <h2>About Me</h2>
  <p> I am a final-year Mphil candidate at The Chinese University of Hong Kong (Shenzhen) advised by Prof. <a href="https://scholar.google.com/citations?user=0TTt3QsAAAAJ&hl=en&oi=ao">Zhen Li</a>. 
  Before that, I received my bachelor's degree in Law and Business Administration from Nankai University. I plan to enter the job market in 2026 and am open to overseas opportunities in AI/Technology Strategy roles. Feel free to contact me early to discuss potential collaborations.
  <!-- I am looking for PhD position for 2026. -->
    <!-- I previously worked as a research intern at Tencent, Shanghai AI Laboratory, Amazon AWS AI Lab and Roblox. -->
    <!-- Currently, I am leading the <del>high-level vision</del> language model group at <a href="https://bivlab123.github.io/">USTC-BIVLab</a>. -->
  <!-- </p> -->
  <!-- <p>  -->
  <!-- My research interests mainly include <b>Multi-modal Learning, Large Language/Vision Models</b>, and <b>3D Vision.</b>  -->
  <!-- </p> -->
  <!-- <p> I am also the Co-founder of <a href="./files/wechat.jpg">Z Potentials</a> which aims to harness the power of the latest technological waves, including AI, XR, Robotics, to empower and inspire the youth of today. Please contact me if you have any questions or want to collaborate.  -->
  
  <!-- <p>  -->
      <!-- (<i>Wechat: <a>nature_claireG</a></i>)</p> -->
  <!-- <p><b>‚ú® NOTE</b>: <i>Our Lab <a href="https://bivlab123.github.io/">[Link]</a> is looking forward to having elegant students or researchers join us. Positions for Master‚Äôs, Ph.D., and post-doc are opening. If you are interested in our research and want to join us, just email me!</i></p> -->
  <!-- <p><b>üî• NOTE</b>: <i>Our team <a href="https://team.doubao.com/zh/direction/llm">(Seed LLM)</a> is looking forward to having elegant researchers join us. Intern and FTE positions are opening. If you are interested in LLM reasoning/agent, feel free to contact me~</i></p> -->
<!-- <h2>News</h2>
<ul>
  <li>
    [2024.7] We released <a href="https://mindsearch.netlify.app/">MindSearch(ÊÄù¬∑Á¥¢)</a>, which is an awesome AI search engine comparable to Perplexity.ai Pro. Welcome to use and provide your feedback!
  </li>
  <li>
    [2024.3] We released <a href="https://github.com/InternLM/Agent-FLAN">Agent-FLAN</a>, which explores the construction of high-quality agent corpus for LLMs.
  </li>
  <li>
    [2023.12] We released <a href="https://github.com/open-compass/T-Eval">T-Eval</a>, a step-by-step evaluation benchmark to gauge your LLMs on tool utilization.
  </li>
  <li>
    [2023.8] We released <a href="https://github.com/InternLM/lagent.git">LAgent</a>, an extremely simple LLM agent framework. Welcome to use and provide your feedback!
  </li>
</ul> -->

<!-- <h2>Experience</h2>
<ul>
  <li>
    Sep.2019 - Jul.2020, <i>Perception Research Intern</i>, <b>TuSimple</b>
  </li>

  <li>
    Oct.2020 - Mar.2021, <i>Computer Vision Intern</i>, <b>ByteDance</b>
  </li>

  <li>
    Mar.2021 - Aug.2023, <i>Perception Research Intern</i>, <b>SenseTime</b>
  </li>

  <li>
    Aug.2023 - Aug.2024, <i>LLM Research Intern</i>, <b>Shanghai AI Laboratory</b>
  </li>

  <li>
    Aug.2024 - Now, <i>LLM Research Intern</i>, <b>ByteDance</b>
  </li>

</ul> -->

<!-- <h2>Awards</h2>
<ul>
  <li>
    National Scholorship. 2024.
  </li>
  <li>
    <b>1<sup>st</sup></b> place at VCL2023 Challenge, Multitask Learning for Robustness Track! (<b>ICCV 2023</b> Workshop)
  </li>
  <li>
    <b>1<sup>st</sup></b> place at VCL2023 Challenge, CTTA for Semantic Segmentation Track! (<b>ICCV 2023</b> Workshop)
  </li>
  <li>
    <b>2<sup>nd</sup></b> place at VCL2023 Challenge, CTTA for Object Detection Track! (<b>ICCV 2023</b> Workshop)
  </li>
  <li>
    National Scholarship. 2022.
  </li>
  <li>
    <b>3<sup>rd</sup></b> place at SSLAD 2022 Challenge, 3D Object Detection Track! (<b>ECCV 2022</b> Workshop)
  </li>
  <li>
    <b>2<sup>nd</sup></b> place at Mobile AI 2022 Challenge, Monocular Depth Estimation Track! (<b>ECCV 2022</b> Workshop)
  </li>
  <li>
    National Scholarship. 2021.
  </li>
  <li>
    <b>2<sup>nd</sup></b> place at Streaming Detection Challenge, Full Stack Track! (<b>CVPR 2021</b> Workshop)
  </li>
  <li>
    <b>3<sup>rd</sup></b> place at UG2+ Challenge, Low-Light Face Detection Track! (<b>CVPR 2021</b> Workshop)
  </li>
  <li>
    <b>1<sup>st</sup></b> place at 3D FUTURE Challenge, Instance Segmentation Track! (<b>IJCAI 2020</b> Workshop)
  </li>
  <li>
    <b>1<sup>st</sup></b> place at Waymo Open Challenge, 2D Detection Track! (<b>CVPR 2020</b> Workshop)
  </li>
</ul> -->



<h2>Selected Publications and Preprints</h2>
* denotes equal contribution. 
<!-- &diams; denotes project leader. -->
<!-- <h3>Preprint Papers</h3>
<table class="pub_table">
<tbody>
  <tr>
    <td class="pub_td2"><b>Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training</b>
      <br>Siyu Yuan*, <u>Zehui Chen*</u>, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen
      <br>Arxiv, 2025</i><br>
      [<a href="https://arxiv.org/pdf/2501.11425">PDF</a>]
      [<a href="https://github.com/bytedance/Agent-R">Code</a>]
    </td>
  </tr>
</tbody>
</table> -->
<table class="pub_table">
<tbody>
<!-- <h3>Published Papers</h3> -->
  <!-- <tr><td><b><font color="##36BC2">&spades; (Co-) First author Papers </font></b></td></tr> -->
  <tr>
    <td class="pub_td2"><b>PiSA: A Self-Augmented Data Engine and Training Strategy for 3D Understanding with Large Models</b>
      <br><u>Zilu Guo</u>, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, Zhen Li
      br><i>IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2026</i><br>
      [<a href="https://arxiv.org/pdf/2503.10529">PDF</a>]
      [<a href="https://github.com/CG-ops/PiSA">Code</a>]
    </td>
  </tr>
  
  <tr>
    <td class="pub_td2"><b>DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation</b>
      <br>Hongbin Lin, <u>Zilu Guo</u>, Yifan Zhang, Shuaicheng Niu, Yafeng Li, Ruimao Zhang, Shuguang Cui, Zhen Li
      <br><i>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025</i><br>
      [<a href="">PDF</a>]
      <!-- [<a href="https://mindsearch.netlify.app/">Project</a>] -->
      [<a href="https://github.com/Hongbin98/DriveGEN">Code</a>]
    </td>
  </tr>
  
  <tr>
    <td class="pub_td2"><b>MME-CoT: Benchmarking CoT in MLLMs for Reasoning Quality, Robustness, and Efficiency</b>
      <br>Dongzhi Jiang*, Renrui Zhang*, Ziyu Guo, Jianhan Jin, Liuhui Wang, Yu Qi, Xinyan Chen, <u>Claire Guo</u>, Yanwei Li, Bo Zhang, Chaoyou Fu, Shen Yan, Hongsheng Li
      <br>International Conference on Machine Learning (<b>ICML</b>), 2025</i><br>
      [<a href="https://arxiv.org/pdf/2502.09621">PDF</a>]
      [<a href="https://github.com/CaraJ7/MME-CoT">Code</a>]
    </td>
  </tr>

  <tr>
    <!-- <td class="pub_td1"><img src="./files/graphdetr4d.png" class="papericon"></td> -->
    <td class="pub_td2"><b>Achieving flexible fairness metrics in federated medical imaging</b>
      <br>Mr Huijun Xing, Mr Rui Sun, Professor Jinke Ren, Dr jun Wei, Dr Chun-Mei Feng, Ms xuan Ding, <u>Ms Zilu Guo</u>, Ms Yu Wang, yudong Hu, Dr WeiWei, Dr Xiaohua Ban, Chuanlong Xie, Mr Yu Tan, Dr Xian Liu, Professor Shuguang Cui, Professor Xiaohui Duan, Professor Zhen Li
      <br><i><b>Nature Communications</b>, 2025</i>
      <br>
      [<a href="https://www.nature.com/articles/s41467-025-58549-0">PDF</a>]
      [<a href="">Code</a>]
    </td>
  </tr>

  <!-- <tr>
     <td class="pub_td1"><img src="./files/graphdetr4d.png" class="papericon"></td> 
    <td class="pub_td2"><b>Differential evolution algorithm of adaptive hybrid grey wolf based on dynamic multi-population</b>
      <br>Ren X, <u>Guo Z</u>
      <br>Arxiv, 2023<i>
      <br>
      [<a href="">PDF</a>]
      [<a href="">Code</a>]
    </td>
  </tr> -->

  <tr>
    <td class="pub_td2"><b>VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</b>
      <br>Longtian Qiu*, Renrui Zhang*, Ziyu Guo, Ziyao Zeng, <u>Zilu Guo</u>, Yafeng Li, Guangnan Zhang
      <br>Arxiv, 2021</i><br>
      [<a href="https://arxiv.org/pdf/2112.02399">PDF</a>]
      [<a href="">Code</a>]
    </td>
  </tr>
  </tbody>
</table>

<!-- <h2>Interviews in Tech Media</h2>
<table class="pub_table">
<tbody>
  <tr>
    <td class="pub_td2"><b>Z Potentials | From DeepSeek to Northwestern, Zihan Wang Unveils How RL and MoE Ignite a Revolution in LLM</b>
      <br>DeepSeek Researcher
      [<a href="https://zpotentials.substack.com/p/z-potentialsex-deepseek-ai-reseacher">Link</a>]
    </td>
  </tr>
  <tr>
    <td class="pub_td2"><b>Z Potentials | Xingyao Wang: 99-born PhD Founder in AI Programming, Backed by Anthropic, Peaked on Global Authority List, Solving Over Half of Coding Challenges</b>
      <br>Co-founder of All Hands AI
      [<a href="https://open.substack.com/pub/zpotentials/p/z-potentials-xingyao-wang-99-born?utm_campaign=post&utm_medium=web">Link</a>]
    </td>
  </tr>
  <tr>
    <td class="pub_td2"><b>Z Potentials | Xiaoyin Qu: From Stanford Dropout to Successful Startup Exit‚ÄîNow Pioneering Heeyo, an AI Education Tool Backed by OpenAI</b>
      <br>CEO and Founder of Heeyo
      [<a href="https://zpotentials.substack.com/p/z-potentials-xiaoyin-qu-from-stanford">Link</a>]
    </td>
  </tr>
  <tr>
    <td class="pub_td2"><b>Z Potentials | Gonglue JIANG: Harvard Graduate, Ex-Googler and Founder of Smart Glasses, Surpassing Ray-Ban and Meta on Amazon Best-Seller List</b>
      <br>CEO of VITURE
      [<a href="https://zpotentials.substack.com/p/z-potentials-gonglue-jiang-harvard">Link</a>]
    </td>
  </tr>
  <tr>
    <td class="pub_td2"><b>Z Potentials | Akool, an AI video tool created by Jiajun Lv, has rapidly gained popularity in North America, reaching an annual recurring revenue (ARR) of $40 million in just two years</b>
      <br>The Creator of Akool
      [<a href="https://zpotentials.substack.com/p/z-potentials-akool-the-creator-of">Link</a>]
    </td>
  </tr>
</table> -->
  
<h2>Gallery</h2>
<!-- <p> Here are a few of my personal favorite photos taken over the years. I hope you like them.</p> -->
<!-- <p> In case your life happens to be stressful at the moment, I wish they could bring you a moment with some inner peace. -->
  <!-- , which sadly seems to always be the case for most of us -->
  
</p>
  <table border="0" cellspacing="10" cellpadding="0">
    <tr>
      <!-- <td>
        <div>
          <img width="260" src="./files/zhuyeimg.jpg" border="0">
          <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> 
        </div>
      </td> -->
      <td>
        <div>
          <img width="260" src="./files/3.pic.jpg" border="0">
          <!-- <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> -->
        </div>
      </td>
      <td>
        <div>
          <img width="260" src="./files/1.pic.jpg" border="0">
          <!-- <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> -->
        </div>
      </td>
      <td>
        <div>
          <img width="150" src="./files/2.pic.jpg" border="0">
        </div>
      </td>
      <td>
        <div>
          <img width="200" src="./files/4.pic.jpg" border="0">
          <!-- <p style="font-size: 14px; text-align: center;">Generated by MidJourney and prompted with my selfie.</p> -->
        </div>
      </td>
    </tr>
  </table>
  


</div>
</div>


</body></html>
